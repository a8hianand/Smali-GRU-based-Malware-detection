
import random
import math
import re
import csv
import os
import pandas as pd
import sklearn
from sklearn import metrics
from sklearn.utils import shuffle
import numpy as np 
import matplotlib.pyplot as plt
import seaborn as sns
import keras
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from keras.models import load_model
from keras.models import Model
from keras.layers import GRU, Dense, Activation, Input, Embedding, Dropout
from keras.optimizers import RMSprop
from keras.preprocessing.text import Tokenizer
from keras.preprocessing import sequence
from keras.utils import to_categorical, pad_sequences
from keras.callbacks import EarlyStopping


# In[3]:


mfiles=[]
bfiles=[]


# In[4]:


for file in os.listdir('C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Scripts\\MProjSmall(new)\\ProcessedDataset'):
    if file.startswith("drebin"):
        mfiles.append(file)
x=len(mfiles)


# In[5]:


for file in os.listdir('C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Scripts\\MProjSmall(new)\\ProcessedDataset'):
    if file.startswith("benign"):
        bfiles.append(file)
y=len(bfiles)


# In[6]:


for i in mfiles:
    fname=str(i)
    data=[]
    path1='C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Scripts\\MProjSmall(new)\\ProcessedDataset\\'+fname
    with open(path1, 'r') as fr:
        data=fr.read()
    path2='C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Scripts\\MProjSmall(new)\\Malware\\'+fname      
    with open(path2, 'w') as fr:
        fr.write(data)


# In[7]:


for i in bfiles:
    fname=str(i)
    data=[]
    path1='C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Scripts\\MProjSmall(new)\\ProcessedDataset\\'+fname
    with open(path1, 'r') as fr:
        data=fr.read()
    path2='C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Scripts\\MProjSmall(new)\\Benign\\'+fname      
    with open(path2, 'w') as fr:
        fr.write(data)


# In[8]:


total=x+y
training_m_l=math.ceil(0.7*x)
testing_m_l=x-training_m_l
training_b_l=math.ceil(0.7*y)
testing_b_l=y-training_b_l
training_files=mfiles[:training_m_l]+bfiles[:training_b_l]
testing_files=mfiles[training_m_l:]+bfiles[training_b_l:]


# In[9]:


print("Total Files:", total)
print("Malware Files:",x)
print("Benign Files:",y)
print("----------------------------------------------")
print("Training Files:", training_l)
print("Training Malware Files:", training_m_l)
print("Training Benign Files:", training_b_l)
print("----------------------------------------------")
print("Testing Files:", testing_l)
print("Testing Malware Files:", testing_m_l)
print("Training Benign Files:", testing_b_l)


# In[10]:


trainparts = {}
for file in training_files:
    path = 'C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Scripts\\MProjSmall(new)\\ProcessedDataset\\'+file
    with open(path, 'r') as f:
        parts = f.read().split('\n')
        trainparts[file] = parts


# In[11]:


testparts = {}
for file in testing_files:
    path = 'C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Scripts\\MProjSmall(new)\\ProcessedDataset\\'+file
    with open(path, 'r') as f:
        parts = f.read().split('\n')
        testparts[file] = parts


# In[17]:


def extract_features(dataset):
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts([item for sublist in dataset.values() for item in sublist])
    sequences = tokenizer.texts_to_sequences([item for sublist in dataset.values() for item in sublist])
    word_index = tokenizer.word_index
    vocab_size = len(word_index) + 1
    max_len = max(len(seq) for seq in sequences)
    data = pad_sequences(sequences, maxlen=max_len)
    labels = [1 if 'drebin' in key else 0 for key in dataset.keys() for _ in dataset[key]]
    labels = to_categorical(labels, num_classes=2)
    return data, labels, vocab_size, max_len

trainx, trainy, vocab_size, max_len = extract_features(trainparts)
testx, testy, _, _ = extract_features(testparts)


# In[21]:


inputs = Input(name='inputs', shape=[max_len])
layer = Embedding(vocab_size, 50, input_length=max_len)(inputs)
layer = GRU(64)(layer)
layer = Dense(256, name='FC1')(layer)
layer = Activation('relu')(layer)
layer = Dropout(0.5)(layer)
layer = Dense(2, name='out_layer')(layer)
layer = Activation('softmax')(layer)
model = Model(inputs=inputs, outputs=layer)

model.summary()
model.compile(loss='categorical_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])

history = model.fit(trainx, trainy, batch_size=128, epochs=1, validation_split=0.2)

# Save the model
model.save('GRU_model.h5')


# In[19]:


test_loss, test_accuracy = model.evaluate(testx, testy)
print(f'Test Accuracy: {test_accuracy}')


# In[15]:


predictions = model.predict(testx)
predicted_classes = np.argmax(predictions, axis=1)
true_classes = np.argmax(testy, axis=1)


# In[22]:


preds = model.predict(testx)
preds = np.argmax(preds, axis=1)
testy_labels = np.argmax(testy, axis=1)

dfpreds = pd.DataFrame(columns=['Fname', 'PredictedCategory'])
index = 0
finalf = {}
for k, v in testparts.items():
    finalf[k] = preds[index]
    index += 1

dfactual = pd.DataFrame(columns=['Fname', 'ActualCategory'])
index = 0
for k, v in testparts.items():
    actual = 1 if 'drebin' in k else 0
    for _ in v:
        dfactual = dfactual.append({'Fname': k, 'ActualCategory': actual}, ignore_index=True)
        index += 1

print("Evaluating Model:")
score = model.evaluate(testx, testy)

cm = metrics.confusion_matrix(dfactual.ActualCategory, dfpreds.PredictedCategory)

cmap = sns.color_palette("pastel", as_cmap=True)
ax = sns.heatmap(cm / np.sum(cm, axis=1, keepdims=True), annot=True, cmap=cmap, fmt='.2f')
ax.xaxis.tick_top()
ax.set_title('Confusion Matrix\n')
ax.set_xlabel('\nPredicted values')
ax.set_ylabel('Actual Values ')
ax.xaxis.set_ticklabels(['Benign', 'Malware'])
ax.yaxis.set_ticklabels(['Benign', 'Malware'])
plt.show()




